{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC_CD4QVpv2y"
      },
      "source": [
        "$\\textbf{Libraries}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0qTVSPlwpWxL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shFgMaac_lg5"
      },
      "source": [
        "$\\textbf{Data Processing}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LHeVuqshpWxP"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"raw.csv\")\n",
        "\n",
        "# Step 1: Separate the data based on labels\n",
        "label_1_data = data[data['hit'] == 1]\n",
        "label_0_data = data[data['hit'] == 0]\n",
        "\n",
        "# Step 2: Sample from the label 0 data to match the number of label 1 points\n",
        "balanced_label_0_data = label_0_data.sample(n=len(label_1_data), random_state=42)\n",
        "\n",
        "# Step 3: Concatenate the balanced subsets\n",
        "data = pd.concat([label_1_data, balanced_label_0_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmAVURwppWxQ"
      },
      "source": [
        "$\\textit{Mapping the alleles to corresponding pseudo sequences}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tk50DWt-pWxR"
      },
      "outputs": [],
      "source": [
        "# Getting the pseudo sequences\n",
        "pseudo_seq=pd.read_csv('MHC_pseudo.dat',delimiter=r\"\\s+\",header=None,names=['allele','Pseudo_Sequence'])\n",
        "\n",
        "# Normalizing the pseudo sequences HLA-[gene][allele_group]:[protein_code] -> HLA-[gene]*[allele_group]:[protein_code]\n",
        "def regularize(allele):\n",
        "    if allele[:3]=='HLA':\n",
        "        allele=allele[:5]+'*'+allele[5:]\n",
        "    return allele\n",
        "pseudo_seq['allele']=pseudo_seq['allele'].apply(regularize)\n",
        "\n",
        "# Mapping the alleles to the corresponding pseudo sequences\n",
        "\n",
        "allele2pds={} # allele to pseudo sequence dictionary\n",
        "\n",
        "alleles=data['allele'].unique()\n",
        "for allele_idx in range(len(alleles)):\n",
        "    for pds_idx in range(len(pseudo_seq)):\n",
        "        if pseudo_seq['allele'][pds_idx]==alleles[allele_idx]:\n",
        "            allele2pds[alleles[allele_idx]]=pseudo_seq['Pseudo_Sequence'][pds_idx]\n",
        "            break\n",
        "\n",
        "def allele2pds_fn(allele):\n",
        "    return allele2pds[allele]\n",
        "data['pds']=data['allele'].apply(allele2pds_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqvFm4N4_eP1"
      },
      "source": [
        "$\\textit{Saving}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pJHO27IGpWxT"
      },
      "outputs": [],
      "source": [
        "with open('data.pkl','wb') as file :\n",
        "    pickle.dump(data,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b_slWylLtTiP"
      },
      "outputs": [],
      "source": [
        "with open('data.pkl','rb') as file :\n",
        "  data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du4s7QFRpWxU"
      },
      "source": [
        "$\\textbf{Model}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a4zEJLfepWxU"
      },
      "outputs": [],
      "source": [
        "class MHCBindingModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name='Rostlab/prot_bert',\n",
        "                 embedding_dim=1024,\n",
        "                 hidden_dims=[512, 256],\n",
        "                 dropout_rate=0.3,\n",
        "                 fine_tune= \"qlora\"):\n",
        "        super(MHCBindingModel, self).__init__()\n",
        "        if fine_tune == \"qlora\":\n",
        "          # Quantization configuration\n",
        "          quantization_config = BitsAndBytesConfig(\n",
        "              load_in_4bit=True,\n",
        "              bnb_4bit_compute_dtype=torch.float16,\n",
        "              bnb_4bit_quant_type=\"nf4\",\n",
        "              bnb_4bit_use_double_quant=True,\n",
        "          )\n",
        "\n",
        "          # Load base model with quantization\n",
        "          self.base_model = AutoModel.from_pretrained(\n",
        "              model_name,\n",
        "              quantization_config=quantization_config,\n",
        "              device_map=\"auto\"\n",
        "          )\n",
        "\n",
        "          # Prepare model for kbit training\n",
        "          self.base_model = prepare_model_for_kbit_training(self.base_model)\n",
        "        elif fine_tune == \"lora\":\n",
        "          self.base_model = AutoModel.from_pretrained(model_name)\n",
        "        elif fine_tune ==\"no_lora\":\n",
        "          self.base_model = AutoModel.from_pretrained(model_name)\n",
        "          for param in self.base_model.parameters():\n",
        "            param.requires_grad = True  # Enable gradients\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if fine_tune in {\"lora\",\"qlora\"} :\n",
        "          # Apply LoRA configuration\n",
        "          lora_config = LoraConfig(\n",
        "              r=16,  # Rank of LoRA adaptation\n",
        "              lora_alpha=32,\n",
        "              target_modules=['query', 'value', 'key'],\n",
        "              lora_dropout=0.1,\n",
        "              bias=\"none\",\n",
        "              task_type=\"FEATURE_EXTRACTION\"\n",
        "          )\n",
        "          self.base_model = get_peft_model(self.base_model, lora_config)\n",
        "\n",
        "        # Classification layers\n",
        "        classifier_layers = []\n",
        "        prev_dim = embedding_dim * 2  # Concatenated embeddings\n",
        "\n",
        "        for dim in hidden_dims:\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "\n",
        "        classifier_layers.append(nn.Linear(prev_dim, 1))\n",
        "        classifier_layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def embed_sequence(self, sequence):\n",
        "        # Tokenize sequence\n",
        "        inputs = self.tokenizer(\n",
        "            sequence,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.base_model.device)\n",
        "\n",
        "        # Get embeddings\n",
        "        outputs = self.base_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        return embeddings.squeeze()\n",
        "\n",
        "    def forward(self, mhc_sequence, peptide_sequence):\n",
        "        # Embed both sequences\n",
        "        mhc_embedding = self.embed_sequence(mhc_sequence)\n",
        "        peptide_embedding = self.embed_sequence(peptide_sequence)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        combined_embedding = torch.cat([mhc_embedding, peptide_embedding], dim=-1)\n",
        "\n",
        "        # Classify\n",
        "        binding_probability = self.classifier(combined_embedding)\n",
        "\n",
        "        return binding_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3QLTu0c7jUC"
      },
      "source": [
        "$\\textit{New Dataset Class}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l8zB8YObpWxW"
      },
      "outputs": [],
      "source": [
        "class MHCPeptideDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Initialize dataset from a DataFrame\n",
        "        Expected columns: 'mhc_sequence', 'peptide_sequence', 'binding_label'\n",
        "        \"\"\"\n",
        "        self.mhc_sequences = data['pds'].tolist()\n",
        "        self.peptide_sequences = data['peptide'].tolist()\n",
        "        self.labels = torch.tensor(data['hit'].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mhc_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'mhc_sequences': self.mhc_sequences[idx],\n",
        "            'peptide_sequences': self.peptide_sequences[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMp68WRB7wdS"
      },
      "source": [
        "$\\textit{Training}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7HTt6VBk8OwP"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "okRG72_BvMAA"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "shuffled_data = shuffle(data, random_state=42)\n",
        "\n",
        "p=0.8 # Percentage of train dataset\n",
        "\n",
        "train_data = shuffled_data.iloc[:int(p * len(shuffled_data))]\n",
        "train_data = MHCPeptideDataset(train_data)\n",
        "\n",
        "val_data = shuffled_data.iloc[int(p * len(shuffled_data)):]\n",
        "val_data = MHCPeptideDataset(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "84h7onNnpWxX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    epochs=epochs,\n",
        "    device=device,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train QLoRA MHC Binding Prediction Model\n",
        "\n",
        "    Args:\n",
        "    - model (MHCBindingModel): Initialized MHC binding model\n",
        "    - train_dataset (MHCPeptideDataset): Training dataset\n",
        "    - val_dataset (MHCPeptideDataset, optional): Validation dataset\n",
        "    - batch_size (int): Training batch size\n",
        "    - learning_rate (float): Optimizer learning rate\n",
        "    - epochs (int): Number of training epochs\n",
        "    - device (torch.device, optional): Device to train on\n",
        "    - wandb_logging (bool): Enable Weights & Biases logging\n",
        "    - wandb_project (str): Weights & Biases project name\n",
        "    - wandb_run_name (str, optional): Specific run name\n",
        "\n",
        "    Returns:\n",
        "    - Trained model\n",
        "    - Training history (dict)\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    ) if val_dataset is not None else None\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': []\n",
        "    }\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_acc = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Training\", total=len(train_loader)):\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare batch\n",
        "            mhc_sequences = batch['mhc_sequences']\n",
        "            peptide_sequences = batch['peptide_sequences']\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Compute predicitions\n",
        "            predictions = model(mhc_sequences, peptide_sequences).squeeze().to(device)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            # Compute acc\n",
        "            predicted_labels = (predictions > 0.5).int()\n",
        "            train_acc += (predicted_labels == labels).sum().item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        train_acc = (train_acc*100)/len(train_dataset)\n",
        "\n",
        "        # Average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_data)\n",
        "        print(f\"  Training Loss: {avg_train_loss:.4f}   Training Accuracy: {train_acc:.4f}%\")\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\", total=len(val_loader)):\n",
        "                # Prepare batch\n",
        "                mhc_sequences = batch['mhc_sequences']\n",
        "                peptide_sequences = batch['peptide_sequences']\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "\n",
        "                # Compute predictions\n",
        "                predictions = model(mhc_sequences, peptide_sequences).squeeze().to(device)\n",
        "\n",
        "                # Compute loss\n",
        "                val_loss = criterion(predictions, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                predicted_labels = (predictions > 0.5).int()\n",
        "                val_acc += (predicted_labels == labels).sum().item()\n",
        "\n",
        "\n",
        "        val_acc = (val_acc*100)/len(val_dataset)\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        print(f\"  Validation Loss: {avg_val_loss:.4f}   Validation Accuracy: {val_acc:.4f}%\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXN3AMOKuVRp",
        "outputId": "88239d7d-452b-4f1b-a7f1-ebb841ed7d33"
      },
      "outputs": [],
      "source": [
        "# Baseline finetuning\n",
        "base_model = MHCBindingModel(fine_tune = \"no_lora\")\n",
        "_,history_base =  train(\n",
        "            base_model,\n",
        "            train_data,\n",
        "            val_data,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=lr,\n",
        "            epochs=epochs,\n",
        "            device=device\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6ZLgsafv8DL",
        "outputId": "e9f69a24-ac49-4e92-bbe0-5abe6f519269"
      },
      "outputs": [],
      "source": [
        "# Training with lora weights\n",
        "lora_model = MHCBindingModel(fine_tune = \"lora\")\n",
        "_,history_lora = train(\n",
        "        lora_model,\n",
        "        train_data,\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        epochs=epochs,\n",
        "        device=device\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OdJDdaTKXZEf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpE04oA-9fgI",
        "outputId": "f0436430-478c-4d06-fbe7-df64fed1c596"
      },
      "outputs": [],
      "source": [
        "# Training with QLoRA weights\n",
        "qlora_model = MHCBindingModel(fine_tune = \"qlora\")\n",
        "_,history_qlora= train(\n",
        "      qlora_model,\n",
        "      train_data,\n",
        "      val_data,\n",
        "      batch_size=batch_size,\n",
        "      learning_rate=lr,\n",
        "      epochs=epochs,\n",
        "      device=device\n",
        "      )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
